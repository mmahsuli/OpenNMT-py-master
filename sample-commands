#!/usr/bin/env python3

# preprocess
python3 preprocess.py -train_src data/verbmobil-enfa/train.en -train_tgt data/verbmobil-enfa/train.fa -valid_src data/verbmobil-enfa/dev.en -valid_tgt data/verbmobil-enfa/dev.fa -save_data data/verbmobil-enfa/data -src_seq_length_trunc 0 -tgt_seq_length_trunc 0

####################
# preprocess for shared vocab
python3 preprocess.py -train_src data/verbmobil-enfa/train.en -train_tgt data/verbmobil-enfa/train.fa -valid_src data/verbmobil-enfa/dev.en -valid_tgt data/verbmobil-enfa/dev.fa -save_data data/verbmobil-enfa/data-shared-vocab -src_seq_length_trunc 0 -tgt_seq_length_trunc 0 -share_vocab
# use pretrained embedding from GloVe
python3 tools/embeddings_to_torch.py -emb_file_enc "data/verbmobil-enfa/bilingual-vectors.txt" -emb_file_dec "data/verbmobil-enfa/bilingual-vectors.txt"  -dict_file "data/verbmobil-enfa/data-shared-vocab.vocab.pt" -output_file "data/verbmobil-enfa/embeddings"
#train using pretrained embeddings and fixing the embeddings	
python3 train.py -data data/verbmobil-enfa/data-shared-vocab -save_model model/enfa-verbmobil-baseline-fixed-pretrained-embedding -world_size 1 -gpu_ranks 0 -batch_size 64 -max_generator_batches 120 -valid_steps 5000 -share_embeddings -pre_word_vecs_enc data/verbmobil-enfa/embeddings.enc.pt -pre_word_vecs_dec data/verbmobil-enfa/embeddings.dec.pt -fix_word_vecs_enc -fix_word_vecs_dec -share_embeddings
####################
# train using baseline model
python3 train.py -data data/verbmobil-enfa/data -save_model model/enfa-verbmobil-baseline -world_size 1 -gpu_ranks 0 -batch_size 64 -max_generator_batches 120 -valid_steps 5000

####################
# train using transformer model
python  train.py -data data/verbmobil-enfa/data -save_model model/enfa-verbmobil-transformer -layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 -encoder_type transformer -decoder_type transformer -position_encoding -train_steps 200000  -max_generator_batches 2 -dropout 0.1 -batch_size 4096 -batch_type tokens -normalization tokens  -accum_count 2 -optim adam -adam_beta2 0.998 -decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0  -param_init_glorot -label_smoothing 0.1 -valid_steps 10000 -save_checkpoint_steps 10000 -world_size 1 -gpu_ranks 0
####################
# translate using baseline model
python3 translate.py -model model/enfa-verbmobil-baseline_step_100000.pt -src data/verbmobil-enfa/test.en -tgt data/verbmobil-enfa/ref0  -output pred.txt -replace_unk -verbose -gpu 0

# translate using length model for generator 
python3 translate.py -model model/enfa-verbmobil-baseline_step_100000.pt -src data/verbmobil-enfa/test.en -tgt data/verbmobil-enfa/ref0  -output pred.txt -replace_unk -verbose -gpu 0 -length_model oracle

# translate using length mdoel for reranking the n-best list
python3 translate.py -model model/enfa-verbmobil-baseline_step_100000.pt -src data/verbmobil-enfa/test.en -tgt data/verbmobil-enfa/ref0  -output pred.txt -replace_unk -verbose -gpu 0 -length_model oracle_rerank

